# Automated Agent Evaluation System

This evaluation system provides a comprehensive framework for testing AI agents across three key capabilities: factual question answering, tool usage, and multi-step reasoning.

## Files Created

### 1. `evaluation_dataset.jsonl`
A dataset of 30 carefully crafted tasks:

- **Factual Q&A (10 tasks)**: Mix of short and long context questions with evidence snippets
- **Tool Usage (10 tasks)**: Tasks requiring Tavily search with structured output validation
- **Multi-Step Reasoning (10 tasks)**: Problems requiring 2-4 logical steps with key intermediate values

Each task includes:
- `id`: Unique identifier
- `task`: The question or prompt
- `expected`: Expected answer or output template
- `eval_type`: "factual", "tool", or "reasoning"
- `metadata`: Type-specific evaluation criteria

### 2. `automated_evaluator.mts`
Full-featured evaluator that imports existing agents (ReAct, ToT, Reflect) and provides:

- **Factual Evaluation**: Exact string matching + citation hit rate tracking
- **Tool Evaluation**: Search invocation detection + structured output validation
- **Reasoning Evaluation**: LLM-based assessment of logical steps and intermediate values

### 3. `automated_evaluator_simple.mts`
Simplified version that works without better-sqlite3 dependency:

- Self-contained ReAct agent implementation
- Same evaluation logic as full version
- Tested and working (requires valid API keys)

## Evaluation Metrics

### Factual Q&A
- **Exact Match**: Case-insensitive substring matching
- **Citation Hit Rate**: Percentage of responses containing evidence snippets

### Tool Usage
- **Tool Invocation**: Detects if Tavily search was called
- **Field Presence**: Validates required output fields are present
- **Tolerance Check**: Verifies numeric values within specified ranges

### Multi-Step Reasoning
- **LLM Assessment**: GPT-4o-mini judges reasoning correctness
- **Key Steps Detection**: Identifies required logical steps
- **Intermediate Values**: Tracks presence of expected intermediate results

## Usage

1. **Set API Keys**: Update the API key placeholders in the evaluator files
2. **Run Full Evaluation**: `npx tsx automated_evaluator.mts`
3. **Run Simple Test**: `npx tsx automated_evaluator_simple.mts`

## Output

The system generates:
- **Console Summary**: Real-time progress and results table
- **CSV Report**: Detailed per-task results in `evaluation_results_auto.csv`
- **Performance Metrics**: Success rates, timing, and type-specific statistics

## Example Results

```
üìä EVALUATION RESULTS
================================================================================

| Task ID | Task Type | Agent | Success | Time | Response Preview |
|---------|-----------|-------|---------|------|------------------|
| 1 | factual | ReAct | ‚úÖ | 0.25s | Paris is the capital of France... |
| 2 | tool | ReAct | ‚úÖ | 1.2s | Search results: {"temperature": "22¬∞C"... |
| 3 | reasoning | ReAct | ‚ùå | 0.8s | Let me calculate step by step... |
```

## Key Features

- **Comprehensive Coverage**: Tests all major agent capabilities
- **Automated Assessment**: No manual evaluation required
- **Detailed Metrics**: Granular success/failure analysis
- **Extensible Design**: Easy to add new task types or agents
- **Production Ready**: Handles errors gracefully and provides detailed logging

The system is ready for production use once API keys are configured.
